PySpark Tutorial
================
PySpark is the Spark Python API.  The purpose of PySpark tutorial is to 
provide basic distributed algorithms using PySpark. Note that PySpark is 
an interactive shell for basic testing and debugging and is not supposed 
to be used for production environment.

[Basics of PySpark](./howto/README.md) 
======================================


PySpark Examples and Tutorials
==============================
* wordcount: classic word count
* bigrams: find frequency of bigrams
* basic-join: basic join of two relations R(K, V<sub>1</sub>), S(K,V<sub>2</sub>)
* basic-map: basic mapping of RDD elements
* basic-add: how to add all RDD elements together
* basic-multiply: how to multiply all RDD elements together
* top-N: find top-N and bottom-N
* combine-by-key: find average by using combineByKey()
* basic-filter: how to filter RDD elements
* basic-average: how to find average
* cartesian: rdd1.cartesian(rdd2)
* basic-sort: sortByKey ascending/descending

[How to Minimize the Verbosity of Spark](./howto/minimize_verbosity.md) 
=======================================================================

Questions/Comments
==================
* [View Mahmoud Parsian's profile on LinkedIn](http://www.linkedin.com/in/mahmoudparsian)
* Please send me an email: mahmoud.parsian@yahoo.com
* [Twitter: @mahmoudparsian](http://twitter.com/mahmoudparsian) 

Thank you!

````
best regards,
Mahmoud Parsian
````

[![Data Algorithms Book](https://github.com/mahmoudparsian/data-algorithms-book/blob/master/misc/data_algorithms_image.jpg)](http://shop.oreilly.com/product/0636920033950.do) 
